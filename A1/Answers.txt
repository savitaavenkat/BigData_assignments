Q1. I'm clear with WordCount on the whole.

Q2. The output will now have an output file per reducer (which is 3) and each file shall contain the reduced value of the input set given to it (reducer) (I.e.) [key, count]. And, a key will not re-appear in any of the other output files (all matching keys will be combined to form one [Key, count] pair in an output file- depending on which reducer got the key). In this case (using 3 reducers), we can see that the time taken to reduce is "more" than what it takes for a single reducer (default value if nothing specified) takes. So, when the input to the reducer is a large data set (in the case where there is going to be greatly distinct set of keys present in the input to reducer): the increase (n) in the reducer count will help to process input data in parallel rather than making a chunk of data to be held back for a reducer process to get freed up. As in the case of large data inputs to reducer: using increased number of reducers (but not too high a number, a number that would make it look efficient) will make the time taken per process minimum over- using a single reducer. But, here sine we are using a limited data set and we cannot possibly produce a lot of variation in English words (key)- the idea to use 3 reducers is not useful (time taken per process is going to be greater when compared to that taken by a single reducer).

Q3. The output when the number of reducers is set to 0:  happens to be that the mappers (possibly 1 per input file here) dumped their output as such to the respective output files. By that I mean- there is no reduce happening (not even combine- as we haven't explicitly implemented a combiner in the WordCount problem to combine a mapper's output and then give it to the reducer), the output looks like a series of [key, 1] pairs and the important part is that every output file may have the same keys (no sorting based on keys happen) across each file, provided if the respective input files have the same set of keys in them. Also, you will notice that the name of the output files (4 files, one output file per mapper) generated would hold "m" instead of "r" in them to state that- it's the mapper's output.

Q4. There was a difference in milliseconds seen (which I cannot call as a visibly noticeable difference; used a counter instead) between running the RedditAverage program with and without combiner optimization. I could see that with the combiner, the time taken and the memory used up is slightly higher (very small difference) for the same input. As the data set size isn't really BIG enough (possibly not having HUGE Variations for the key value) for a combiner to optimize the time taken (I mean time taken = voluminous process completed per second), the above result is observed for the given input.  
